<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Zeeshan Khan - Home</title>
  <meta name="description" content="Zeeshan Khan">
  <link rel="stylesheet" href="http://localhost:4000/css/main.css">
  <link rel="canonical" href="http://localhost:4000/">
<link rel="shortcut icon" type ="image/x-icon" href="http://localhost:4000/images/favicon.ico">



</head>


  <body>

    <div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container-fluid">
	<div class="navbar-header">
	  <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar-collapse-1" aria-expanded="false">
		<span class="sr-only">Toggle navigation</span>
		<span class="icon-bar"></span>
		<span class="icon-bar"></span>
		<span class="icon-bar"></span>
	  </button>

    <a class="navbar-brand" href="http://localhost:4000/">Zeeshan Khan</a>
	</div>
	<div class="collapse navbar-collapse" id="navbar-collapse-1">
	  <ul class="nav navbar-nav navbar-right">
		<li><a href="http://localhost:4000/">Home</a></li>
		<li><a href="http://localhost:4000/publications">Publications</a></li>
	  </ul>
	</div>
  </div>
</div>


    <div class="container-fluid">
      <div class="row">
        <div id="gridid" class="col-sm-12">
  <div class="container-fluid">

  <div class="row">

    <div class="col-sm-8">

      <p>I am an MS by research student at <a href="http://cvit.iiit.ac.in">CVIT, IIIT Hyderabad</a> deeply interested in Multimodal Deep Learning. Currently, I am working on dense video understanding involving vision, language, and people in challenging situations like movies. I am also working on Multilingual and Multimodal Neural Machine Translation. I am advised by <a href="https://faculty.iiit.ac.in/~jawahar/index.html">C.V. Jawahar</a>, <a href="https://makarandtapaswi.github.io">Makarand Tapaswi</a> and, <a href="https://vinaypn.github.io"> Vinay Namboodiri</a> at <a href="https://www.iiit.ac.in">IIIT Hyderabad</a>. Prior to this I was a Research Assistant in the Computer Vision lab at <a href="https://iitgn.ac.in">IIT Gandhinagar</a>, where I worked in Computational Photography and Generative Modelling, I was advised by <a href="https://people.iitgn.ac.in/~shanmuga/">Prof. Shanmuganathan Raman</a>.</p>

      <p>With a diverse research experience in multiple domains, I aim at building machines with multimodal sensing abilities that can understand and connect fine-grained aspects of long videos, like objects, humans, actions, intentions, location, emotions, and story plots.</p>

      <p align="center">
  <a href="./docs/Zeeshan_cv_2022.pdf">CV</a> /
  <a href="https://scholar.google.com/citations?user=uvhBVYoAAAAJ&amp;hl=en">Google Scholar</a> /
  <a href="https://github.com/zeeshank95">Github</a> /
  <a href="https://www.linkedin.com/in/khan-zeeshan-606-">LinkedIn</a> /
</p>

      <h3 id="news">News</h3>
      <hr />

      <p>September, 2022 :
<em>One paper accepted to <a href="https://neurips.cc">NeurIPS 2022</a>! We formulate a new structured framework for dense video understanding and propose a Transformer based model, VideoWhisperer that operates on a group of clips and jointly predicts all the salient actions, Semantic roles via captioning and, spatio temporal grounding in a weakly supervised setting.</em></p>

      <p>April, 2022 :
<em>Two papers accepted to <a href="https://www.icpr2022.com">ICPR 2022</a>!, The first one is the first attempt towards generating high speed high dynamic range videos from low speed low dynamic range videos, The second one is on identity aware person image generation in novel poses</em></p>

      <p>August, 2021 :
<em>joining IIIT Hyderabad as a full time MS by research student at CVIT, I will be advised by <a href="https://faculty.iiit.ac.in/~jawahar/index.html">Prof. C.V. Jawahar</a></em></p>

      <p>May, 2021 :
<em>One Paper accepted to <a href="https://2021.aclweb.org">ACL 2021 </a> ! (findings), we propose to recursively prune and retrain a Transformer and find language dependent submodules to overcome negative interference in Multilingual Neural Machine translation</em></p>

      <p>Oct, 2020 :
<em>One Paper accepted to <a href="https://aclanthology.org/volumes/2020.icon-main/"> ICON 2020</a>! we address the task of improving pair-wise machine translation for low resource Indian languages using a filtered back-translation process and subsequent fine-tuning on the limited pair-wise language corpora</em></p>

      <h4 id="see-all-news"><a href="http://localhost:4000/allnews.html">See all news</a></h4>

    </div>

    <div class="col-sm-4" style="display:table-cell; vertical-align:middle; text-align:left">

      <ul style="overflow: hidden">
  <img src="http://localhost:4000/images/profile_pic.jpeg" class="img-responsive" width="100%" />
  </ul>

      <p><!-- <br clear="all" /> --></p>

      <p><a href="mailto:zeeshan.khan@research.iiit.ac.in">zeeshan.khan@research.iiit.ac.in</a> <br />
  CVIT-IIIT Hyderabad, India.<br /></p>

    </div>

  </div>
</div>

<div class="col-sm-12">

  <h3 id="publications">Publications</h3>
  <hr />

  <div class="col-sm-11 clearfix">
    <div class="well">
      <pubtit>Grounded Video Situtation Recognition</pubtit>

      <p><img src="http://localhost:4000/images/pubpic/Neurips.jpg" class="img-responsive" width="250px" style="float: left" /></p>

      <p>We formulate a new structured framework for dense video understanding and propose a Transformer based model, VideoWhisperer that operates on a group of clips and jointly predicts all the salient actions, Semantic roles via captioning and, spatio temporal grounding in a weakly supervised setting</p>

      <p><em><b>Zeeshan Khan</b>, <a href="https://faculty.iiit.ac.in/~jawahar/index.html">C.V. Jawahar</a>, <a href="https://makarandtapaswi.github.io">Makarand Tapaswi</a></em></p>

      <p><i>In Neural Information Processing Systems (<b>NeurIPS</b>), 2022</i></p>

      <p><a href="https://zeeshank95.github.io/grvidsitu">Paper</a>
 /
 <a href="https://zeeshank95.github.io/grvidsitu">Project Page</a>
 /
 <a href="https://zeeshank95.github.io/grvidsitu">Code (Github)</a></p>

    </div>
  </div>

  <div class="col-sm-11 clearfix">
    <div class="well">
      <pubtit>More Parameters No Thanks!</pubtit>

      <p><img src="http://localhost:4000/images/pubpic/ACL.png" class="img-responsive" width="250px" style="float: left" /></p>

      <p>We propose to recursively prune and retrain a Transformer to find language dependent submodules that involves 2 type of paramteres, 1)Shared multlingual and 2)Unique Language dependent parameters, to overcome negative interference in Multilingual Neural Machine translation.</p>

      <p><em><b>Zeeshan Khan</b>, Kartheek Akella, <a href="https://vinaypn.github.io"> Vinay Namboodiri</a>, and <a href="https://faculty.iiit.ac.in/~jawahar/index.html"> C.V. Jawahar </a></em></p>

      <p><i>In Association For Computational Linguistics (<b>ACL</b>) (Findings), 2021</i></p>

      <p><a href="https://aclanthology.org/2021.findings-acl.9/">Paper</a>
 /
 <a href="http://cvit.iiit.ac.in/research/projects/cvit-projects/more-parameters-no-thanks">Project Page</a>
 /
 <a href="https://github.com/zeeshank95/PF-Adaptation">Code (Github)</a></p>

    </div>
  </div>

  <div class="col-sm-11 clearfix">
    <div class="well">
      <pubtit>DeepHS-HDRVideo : Deep High Speed High Dynamic Range Video Reconstruction</pubtit>

      <p><img src="http://localhost:4000/images/pubpic/HDR_video.png" class="img-responsive" width="250px" style="float: left" /></p>

      <p>This is the first attempt towards generating high speed high dynamic range videos from low speed low dynamic range videos. We use video frame interpolation to recursivrly generate the high and low exposure images missing in the input alternative exposure frames. The High and Low exposure frames are merged at each timestep to generate an HDR video.</p>

      <p><em><b>Zeeshan Khan</b>, Parth Shettiwar, Mukul Khanna, <a href="https://people.iitgn.ac.in/~shanmuga/">Shanmuganathan Raman</a></em></p>

      <p><i>In International Conference on Pattern Recognition(<b>ICPR</b>), 2022 <span style="color:red;">(ORAL)</span></i></p>

      <p><a href="https://zeeshank95.github.io/">Paper</a>
 /
 <a href="https://zeeshank95.github.io/">Video</a></p>

    </div>
  </div>

  <div class="col-sm-11 clearfix">
    <div class="well">
      <pubtit>Appearance Consistent Human Pose Transfer via Dynamic Feature Selection</pubtit>

      <p><img src="http://localhost:4000/images/pubpic/Pose_transfer.png" class="img-responsive" width="250px" style="float: left" /></p>

      <p>We present a robut deep architecture for Appearance Consistent person image generation in novel poses. We incorporate a 3 stream network, for image, pose, and appearance. Additionaly we use Gated convolutions and, Non-local attention blocks for generating realistic images.</p>

      <p><em>Ashish Tiwari, <b>Zeeshan Khan</b>, <a href="https://people.iitgn.ac.in/~shanmuga/">Shanmuganathan Raman</a></em></p>

      <p><i>In International Conference on Pattern Recognition (<b>ICPR</b>), 2022</i></p>

      <p><a href="https://zeeshank95.github.io/">Paper</a></p>

    </div>
  </div>

  <div class="col-sm-11 clearfix">
    <div class="well">
      <pubtit>Exploring Pair-Wise NMT for Indian Languages</pubtit>

      <p><img src="http://localhost:4000/images/pubpic/ICON.png" class="img-responsive" width="250px" style="float: left" /></p>

      <p>We address the task of improving pair-wise machine translation for low resource Indian languages using a filtered back-translation process and subsequent fine-tuning on the limited pair-wise language corpora</p>

      <p><em>Kartheek Akella, Sai Himal Allu, Sridhar Suresh Ragupathi, Aman Singhal,<b>Zeeshan Khan</b>, <a href="https://vinaypn.github.io"> Vinay Namboodiri</a>, and <a href="https://faculty.iiit.ac.in/~jawahar/index.html"> C.V. Jawahar </a></em></p>

      <p><i>In International Conference on Natural Language Processing(<b>ICON</b>) 2020</i></p>

      <p><a href="https://aclanthology.org/2020.icon-main.59/">Paper</a></p>

    </div>
  </div>

  <div class="col-sm-11 clearfix">
    <div class="well">
      <pubtit>FHDR: HDR Image Reconstruction from a Single LDR Image using Feedback Network</pubtit>

      <p><img src="http://localhost:4000/images/pubpic/HDR_img.png" class="img-responsive" width="250px" style="float: left" /></p>

      <p>Proposed a recurrent Feedback CNN for HDR image reconstruction from a single exposure LDR image, achieving SOTA results on all the HDR benchmarks. Designed a novel Dense Feedback Block using hidden states of RNN, to transfer the high-level information to the low-level features. LDR to HDR representations are learned in multiple iterations via feedback loops.</p>

      <p><em><b>Zeeshan Khan</b>, Mukul khanna, and <a href="https://people.iitgn.ac.in/~shanmuga/">Prof. Shanmuganathan Raman</a></em></p>

      <p><i>In Global Conference on Signal and Information Processing (<b>GlobalSIP</b>) 2019</i></p>

      <p><a href="https://arxiv.org/pdf/1912.11463.pdf">Paper</a>
 /
 <a href="https://github.com/mukulkhanna/FHDR">Code (Github)</a></p>

    </div>
  </div>

  <p><br clear="all" /></p>

  <h4 id="see-all-publications"><a href="http://localhost:4000/publications">See all publications</a></h4>

</div>


</div>

      </div>
    </div>

    <div id="footer" class="panel">
  <div class="panel-footer">
	<div class="container-fluid">
	  <div class="row">
		<div class="col-sm-5">

		  <p>&copy 2022 Zeeshan Khan. Site made with <a href="https://jekyllrb.com">Jekyll</a>.</p>
		   <p>  </p><p>


		</div>
		<div class="col-sm-5">
		</div>
    <div class="col-sm-5">
		</div>
		<div class="col-sm-5">
		</div>
	  </div>
	</div>
  </div>
</div>

<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
<script src="http://localhost:4000/js/bootstrap.min.js"></script>


  </body>

</html>
