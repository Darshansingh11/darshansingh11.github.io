<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Zeeshan Khan - Grounded VidSitu</title>
  <meta name="description" content="Zeeshan Khan: Grounded VidSitu">
  <link rel="stylesheet" href="http://localhost:4000/css/main.css">
  <link rel="canonical" href="http://localhost:4000/grvidsitu/">
<link rel="shortcut icon" type ="image/x-icon" href="http://localhost:4000/images/favicon.ico">



</head>


  <body>

    <div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container-fluid">
	<div class="navbar-header">
	  <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar-collapse-1" aria-expanded="false">
		<span class="sr-only">Toggle navigation</span>
		<span class="icon-bar"></span>
		<span class="icon-bar"></span>
		<span class="icon-bar"></span>
	  </button>

    <a class="navbar-brand" href="http://localhost:4000/">Zeeshan Khan</a>
	</div>
	<div class="collapse navbar-collapse" id="navbar-collapse-1">
	  <ul class="nav navbar-nav navbar-right">
		<li><a href="http://localhost:4000/">Home</a></li>
		<li><a href="http://localhost:4000/publications">Publications</a></li>
	  </ul>
	</div>
  </div>
</div>


    <div class="container-fluid">
      <div class="row">
        <div id="gridid" class="col-sm-12">
  <h2 align="center">Grounded Video Situation Recognition</h2>

<p style="text-align: center;">
<a href="https://zeeshank95.github.io" style="color: #E71A06; font-size:17px;"> Zeeshan Khan</a>
      
<a href="https://faculty.iiit.ac.in/~jawahar/index.html" style="color: #E71A06; font-size:17px;"> C.V. Jawahar</a>
      
<a href="https://makarandtapaswi.github.io" style="color: #E71A06; font-size:17px;"> Makarand Tapaswi</a>
      
</p>

<p style="text-align: center;"><a href="http://cvit.iiit.ac.in" style="color:#E71A06; font-size:17px;">CVIT, IIIT Hyderabad</a></p>

<p style="text-align: center;"><a href="https://neurips.cc" style="color:#E71A06; font-size:17px;">NeurIPS 2022</a></p>

<p style="text-align: center;">

<a href="https://zeeshank95.github.io/grvidsitu" class="btn">Paper</a>
      

<a href="https://zeeshank95.github.io/grvidsitu" class="btn">Dataset</a>
      

<a href="https://zeeshank95.github.io/grvidsitu" class="btn">Code</a>
      

</p>

<!-- [comment]: ProcedureLearning -->
<!-- <h3>GVSR Task</h3> -->
<h3>GVSR Task</h3>
<div style="text-align: justify">
  <p>GVSR is built on top of <a href="https://vidsitu.org">VidSitu</a>. A large scale dataset containing videos of 10 seconds from complex movie scenes. A video is divded in multiple events of ~2 seconds each. Each event is associated with a salient action verb, each action verb is associated with a semantic role frame eg: agent, patient, tool, location, manner etc. Each role is is annotated using a free form text caption.</p>

  <center>
<figure>
		<div id="projectid">
    <img src="http://localhost:4000/images/pubpic/Neurips.jpg" width="700px" />
		</div>
    <p>&nbsp;</p>
    <figcaption>
      Holistic understanding of video requires to localise and recognise all the salient events, and answer questions, like who did what to whom with what how why and where. GVSR affords this by recognising the action <b>verbs</b>, their corresponding <b>roles</b>, and <b>localising</b> them in the spatio-temporal domain. Entities are coreferenced in all the events.
    </figcaption>
</figure>
</center>

  <!-- The GVSR task involves the following structured peridctions in an end-to-end setting.
- <b>Verb classificatin per event</b>
- <b>Role classification and semantic role labelling through caption generation</b>
- <b>Grounding all the visual semantic roles in the spatio-temporal domain</b> -->

  <p><br /></p>
  <h3 align="center"><span style="color:DodgerBlue">VideoWhisperer</span></h3>
  <!-- <h3 align="center">VideoWhisperer</h3> -->
  <p><br />
<!-- <h3 align="center"><span style="color:DodgerBlue">VideoWhisperer</span></h3> --></p>
  <div style="text-align: justify">

    <center>
<figure>
		<div id="projectid">
    <img src="http://localhost:4000/images/pubpic/Model_Diagram_Final.pdf" width="700px" />
		</div>
    <p>&nbsp;</p>
    <figcaption>
      We propose a new 3-stage Transformer based model for joint structured prediction of Verbs, Role-Captions, and Groundings. Stage-1 learns the contextualised object and event embeddings through a video-object transformerm that is used to predict the verb-role pairs for each event. Stage-2 models all the predicted roles by creating role queries contextualised by event embeddings, and attends to all the object proposals through the role-object transformer decoder, to find the best entity that represents a role. The output embeddings of the roles are fed to the caption generation module. The cross-attention of the role-object decoder ranks all the object proposals enabling localization of each role.
    </figcaption>
</figure>
</center>

    <p><br />
<!-- <h3>VideoWhisperer in action</h3> --></p>
    <h3 align="center"><span style="color:DodgerBlue">VideoWhisperer in action</span></h3>
    <p><br /></p>
    <center>
<figure>
		<div id="projectid">
    <img src="http://localhost:4000/images/pubpic/project_page.gif" width="700px" />
		</div>
    <p>&nbsp;</p>
    <figcaption>
    </figcaption>
</figure>
</center>

    <center>
<figure>
		<div id="projectid">
    <img src="http://localhost:4000/images/pubpic/Arrow.png" width="300px" />
		</div>
    <p>&nbsp;</p>
    <figcaption>
    </figcaption>
</figure>
</center>

    <center>
<figure>
		<div id="projectid">
    <img src="http://localhost:4000/images/pubpic/Neurips_results.png" width="1100px" />
		</div>
    <p>&nbsp;</p>
    <figcaption>
    </figcaption>
</figure>
</center>

    <h3>More Results</h3>
    <div style="text-align: justify">
      <p>For more results on 10 videos <a href="../GVSR.html">Click Here</a></p>

      <h3>BibTeX</h3>
      <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@InProceedings{GrVidSitu2022,
author="Khan, Zeeshan 
and Jawahar, C.V.,
and Tapaswi, Makarand"
title="Grounded Video Situation Recognition",
booktitle = "Neural Information Processing Systems(NeurIPS)",
year="2022"
}

</code></pre></div>      </div>

      <p> </p>
      <p> </p>
    </div>
  </div>
</div>

</div>

      </div>
    </div>

    <div id="footer" class="panel">
  <div class="panel-footer">
	<div class="container-fluid">
	  <div class="row">
		<div class="col-sm-5">

		  <p>&copy 2022 Zeeshan Khan. Site made with <a href="https://jekyllrb.com">Jekyll</a>.</p>
		   <p>  </p><p>


		</div>
		<div class="col-sm-5">
		</div>
    <div class="col-sm-5">
		</div>
		<div class="col-sm-5">
		</div>
	  </div>
	</div>
  </div>
</div>

<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
<script src="http://localhost:4000/js/bootstrap.min.js"></script>


  </body>

</html>
